{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our GeoAI approach to Geodemographic classification consists of four consecutive steps: **Spatial Graph Construction**, **Geo-saptially Embedding Generation**, **Canonical-correlation Analysis-based Embedding generation** and **K-Mean clustering**. This notebook is deonstrating the step of **Canonical-correlation Analysis-based Embedding generation** and **K-Mean clustering**. The steps of **Spatial Graph Construction** and **Geo-saptially Embedding Generation** can be found in file *Step1-GeoAIGeodemographicClassification.ipynb* and *Step2-GeoAIGeodemographicClassification.ipynb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3 and 4**: CorrNet is a machine learning approach for learning common representations from heterogeneous sources of data (i.e., multimodal data). Its architecture is similar to a conventional single-view deep autoencoder but including one encoder-decoder pair for each modality of data. We create a joint representation that maximises the correlation between geographic location (the graph-based embedding produced by GraghSAGE) and census data attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Embedding, Input, LSTM, RepeatVector, Dense, Dropout,concatenate,Conv2D,UpSampling2D,MaxPooling2D,BatchNormalization,Activation,Add,GlobalMaxPool2D\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer, InputSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a canonical-correlation analysis-based loss layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrnetCost(Layer):\n",
    "    def __init__(self,lamda, **kwargs):\n",
    "        super(CorrnetCost, self).__init__(**kwargs)\n",
    "        self.lamda = lamda\n",
    "\n",
    "    def cor(self,y1, y2, lamda):\n",
    "        y1_mean = K.mean(y1, axis=0)\n",
    "        y1_centered = y1 - y1_mean\n",
    "        y2_mean = K.mean(y2, axis=0)\n",
    "        y2_centered = y2 - y2_mean\n",
    "        corr_nr = K.sum(y1_centered * y2_centered, axis=0)\n",
    "        corr_dr1 = K.sqrt(K.sum(y1_centered * y1_centered, axis=0) + 1e-8)\n",
    "        corr_dr2 = K.sqrt(K.sum(y2_centered * y2_centered, axis=0) + 1e-8)\n",
    "        corr_dr = corr_dr1 * corr_dr2\n",
    "        corr = corr_nr / corr_dr\n",
    "        return K.sum(corr) * lamda\n",
    "\n",
    "    def call(self ,x ,mask=None):\n",
    "        h1=x[0]\n",
    "        h2=x[1]\n",
    "\n",
    "        corr = self.cor(h1,h2,self.lamda)\n",
    "\n",
    "        #self.add_loss(corr,x)\n",
    "        #we output junk but be sure to use it for the loss to be added\n",
    "        return corr\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        #print input_shape[0][0]\n",
    "        return (input_shape[0][0],input_shape[0][1])\n",
    "    \n",
    "#ZeroPadding layer is for CorrNet resconstruct from single modality \n",
    "class ZeroPadding(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ZeroPadding, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return K.zeros_like(x)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "def corr_loss(y_true, y_pred):\n",
    "    #print y_true.type,y_pred.type\n",
    "    #return K.zeros_like(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Keras to design the architecture of deep Corrnet. The CorrNet takes input from two modalities of the data and contains three dense layers for each part of encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = 0.02\n",
    "h_loss = 50\n",
    "\n",
    "#Take inputs from two modalities of the data\n",
    "#input tensor to the model, inpx = view1 and inpy = view2\n",
    "inpx = Input(shape=(50,))\n",
    "inpy = Input(shape=(167,))\n",
    "\n",
    "#Three-layer architecture for each part of encoder\n",
    "#Adding dense layers for view1, hx is the hidden representation for view1\n",
    "hx = Dense(32,activation='sigmoid')(inpx)\n",
    "hx = Dense(16, activation='sigmoid',name='hid_l1')(hx)\n",
    "hx = Dense(8, activation='sigmoid',name='hid_l')(hx)\n",
    "\n",
    "#Adding dense layers for view2, hy is the hidden representation for view2\n",
    "hy = Dense(63,activation='sigmoid')(inpy)\n",
    "hy = Dense(32, activation='sigmoid',name='hid_r1')(hy)\n",
    "hy = Dense(8, activation='sigmoid',name='hid_r')(hy)\n",
    "\n",
    "#Combine the ecoded represntations from both encoder\n",
    "h = Add(name='combined_features')([hx,hy]) \n",
    "\n",
    "#Each decoder corresponds to each encoder\n",
    "recx = Dense(50)(h)\n",
    "recy = Dense(167)(h)\n",
    "\n",
    "#Creating a intermediate models\n",
    "branchModel = Model( [inpx,inpy],[recx,recy,h])\n",
    "\n",
    "#reconstruction from view1, view2 = 0-vector\n",
    "[recx1,recy1,h1] = branchModel( [inpx, ZeroPadding()(inpy)])\n",
    "#reconstruction from view2, view1 = 0-vector\n",
    "[recx2,recy2,h2] = branchModel( [ZeroPadding()(inpx), inpy ])\n",
    "\n",
    "#you may probably add a reconstruction from combined\n",
    "[recx3,recy3,h] = branchModel([inpx, inpy])\n",
    "\n",
    "#adding the correlation loss\n",
    "corr=CorrnetCost(-lamda)([h1,h2])\n",
    "\n",
    "#create intermedia model to extract representation\n",
    "feature_extraction = Model([inpx,inpy],h)   \n",
    "model = Model( [inpx,inpy],[recy1,recx2,recx3,recx1,recy2,recy3,corr])\n",
    "model.compile( loss=[\"mse\",\"mse\",\"mse\",\"mse\",\"mse\",\"mse\",corr_loss],optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from each modality as input for the CorrNet. Essentially, they're the geo-saptially aware embeddings created from the step of **Geo-saptially Embedding Generation** and the z-scored census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading geo-saptially aware embeddings created from the step of Geo-saptially Embedding Generation\n",
    "X = np.load('Data/Output/Graph-Embedding/knn8_GraphSAGE.npy')\n",
    "\n",
    "#The step of reading z-scored census is same as when assigning the data to each node as described \n",
    "#in the notbook of GeoAIGeodemographicClassification-Step2\n",
    "colums = pd.read_csv('Data/Input/Census-Data/GreaterLondon_2011_OAC_Raw_uVariables--zscores.csv', nrows=1).columns.tolist()\n",
    "graph_col = colums[1:]\n",
    "node_data = pd.read_csv('Data/Input/Census-Data/GreaterLondon_2011_OAC_Raw_uVariables--zscores.csv',  sep=',', header=None, names=graph_col)\n",
    "\n",
    "node_features = node_data[graph_col]\n",
    "node=np.asarray(node_features.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_l = X\n",
    "X_train_r = node\n",
    "model.fit([X_train_l,X_train_r], [X_train_r,X_train_l,X_train_l,X_train_l,X_train_r,X_train_r,np.zeros((X_train_l.shape[0],h_loss))],\n",
    "                  nb_epoch=300,\n",
    "                  batch_size=140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the joint representation from the intermediate layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraced_feat= feature_extraction.predict([X_train_l,X_train_r])\n",
    "np.save('Data/Output/Corrnet-Embedding/corrnet.npy',extraced_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed the extracted representations to the K-Means clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=8, random_state=0).fit(extraced_feat)\n",
    "labels = pd.DataFrame(kmeans.labels_)\n",
    "#Convert the clusters created by K-Means to csv file\n",
    "labels.to_csv('Data/Output/Geodemographic-Clusters/Clusters.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
