{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeoAIGeodemographicClassification-GCN-SaptiallyEmbeddingGeneration.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates another way of generating node embeddings, Graph Convolutional Neural Network, as a comparison to GraphSAGE. The steps of **Spatial Graph Construction** and **Canonical-correlation Analysis-based Embedding generation** and **K-Mean clustering** can be found in file *Step1-GeoAIGeodemographicClassification-SpatialGraphConstruction.ipynb* and *Step3-GeoAIGeodemographicClassification.ipynb*. \n",
        "\n",
        "**GCN-based embedding generation**: This demo demonstrates how to generate node embeddings using GCN based on the Deep Graph Infomax algorithm. Deep Graph Infomax is a procedure for training graph machine learning models without supervision. In our case presented here, it helps to convert GCN which is commonly a supervised algorthim into an unsupervised learning process to generate node representations. "
      ],
      "metadata": {
        "id": "5JxZvPoVNuTN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4QeMdSIp_G8"
      },
      "source": [
        "from stellargraph.mapper import (\n",
        "    CorruptedGenerator,\n",
        "    FullBatchNodeGenerator,\n",
        "    GraphSAGENodeGenerator,\n",
        "    HinSAGENodeGenerator,\n",
        "    ClusterNodeGenerator,\n",
        ")\n",
        "from stellargraph import StellarGraph\n",
        "from stellargraph.layer import GCN, DeepGraphInfomax\n",
        "import stellargraph as sg\n",
        "from stellargraph import datasets\n",
        "from stellargraph.utils import plot_history\n",
        "import networkx as nx\n",
        "\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading neighbouring information from the saved csv file in *GeoAIGeodemographicClassification-Step1.ipynb*, and converting the neighbouring information into a graph.\n",
        "\n"
      ],
      "metadata": {
        "id": "BlTkIdi1R3fP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2f9DZDXqmOa"
      },
      "source": [
        "#Read neighbouring information from the saved csv file\n",
        "#Specify where the csv file is located\n",
        "edgelist = pd.read_csv('Data/Output/Spatial-Graph/SpatialGraphs.csv')\n",
        "Gnx = nx.from_pandas_edgelist(edgelist, edge_attr=\"neighbouring\")\n",
        "\n",
        "nx.set_node_attributes(Gnx, \"oa\", \"neighbouring\")\n",
        "\n",
        "#Read heading information from the csv file\n",
        "colums = pd.read_csv('Data/Input/Census-Data/GreaterLondon_2011_OAC_Raw_uVariables--zscores.csv', nrows=1).columns.tolist()\n",
        "\n",
        "graph_col = colums[1:]\n",
        "print(graph_col)\n",
        "cols = graph_col+['subject']\n",
        "\n",
        "#Read node values \n",
        "node_data = pd.read_csv('Data/Input/Census-Data/GreaterLondon_2011_OAC_Raw_uVariables--zscores.csv',  sep=',', header=None, names=cols)\n",
        "\n",
        "print(node_data)\n",
        "\n",
        "#Assign z-scored census data to the nodes in the graph\n",
        "node_features = node_data[graph_col]\n",
        "print(node_features)\n",
        "#\n",
        "G = sg.StellarGraph(Gnx, node_features=node_features)\n",
        "\n",
        "print(G.info())\n",
        "nodes = list(G.nodes())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load GCN to take the constructed graph and using *CorruptedGenerator()* function to create the data generators. For details about *CorruptedGenerator()*, please refers to the offical document of [StellarGraph](https://stellargraph.readthedocs.io/en/stable/demos/node-classification/gcn-deep-graph-infomax-fine-tuning-node-classification.html). In a nutshell, *CorruptedGenerator()* returns shuffled node features along with the regular node features and we train our model to discriminate between the two."
      ],
      "metadata": {
        "id": "lbrScQVtSg6P"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64RCUm27r99q",
        "outputId": "cf8cf925-97ac-4c62-dabb-bfa101f17968"
      },
      "source": [
        "fullbatch_generator = FullBatchNodeGenerator(G, sparse=False)\n",
        "gcn_model = GCN(layer_sizes=[128], activations=[\"relu\"], generator=fullbatch_generator)\n",
        "\n",
        "corrupted_generator = CorruptedGenerator(fullbatch_generator)\n",
        "gen = corrupted_generator.flow(G.nodes())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GCN (local pooling) filters...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Deep Graph Infomax to take GCN model and compile the model for \"training\" process. Note that \"training\" here is not the trainig in supervised algorithms. In such an unsupervised algorithm which does not take labels into account, we train our model to discriminate between the shuffled node features and regular node features."
      ],
      "metadata": {
        "id": "6T3PBI1wS4aA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf_yfc5ysKgL",
        "outputId": "ca81065c-c89c-4242-bc46-0c8bb3989c5d"
      },
      "source": [
        "infomax = DeepGraphInfomax(gcn_model, corrupted_generator)\n",
        "x_in, x_out = infomax.in_out_tensors()\n",
        "\n",
        "model = Model(inputs=x_in, outputs=x_out)\n",
        "model.compile(loss=tf.nn.sigmoid_cross_entropy_with_logits, optimizer=Adam(lr=1e-3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model."
      ],
      "metadata": {
        "id": "Z6q-OPWvWUQf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UINLpAOpsPHp",
        "outputId": "cf0610eb-5968-43aa-b798-ec6aae7b8dfc"
      },
      "source": [
        "es = EarlyStopping(monitor=\"loss\", min_delta=0, patience=20)\n",
        "history = model.fit(gen, epochs=100, verbose=1, callbacks=[es])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.9913\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7999\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7869\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7280\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6667\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6297\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6225\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6076\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5731\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5466\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5267\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5210\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5047\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4941\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4730\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4586\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4511\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4441\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4368\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4148\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4051\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3950\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3852\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3765\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3693\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3584\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3446\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3318\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3284\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3166\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3065\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2989\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2857\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2808\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2729\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2675\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2572\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2478\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2418\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2321\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2284\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2245\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2178\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2087\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1995\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1961\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1901\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1808\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1790\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1742\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1675\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1637\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1574\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1526\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1475\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1432\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1441\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1327\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1324\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1335\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1263\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1208\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1166\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1160\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1128\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1140\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1077\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1052\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.1023\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0984\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0938\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0919\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0920\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0909\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0878\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0844\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0816\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0804\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0771\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0783\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0759\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0751\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0703\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0671\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0675\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0665\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0665\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0664\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0648\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0630\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0618\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0622\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0595\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0569\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0566\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0535\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0553\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0538\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0502\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.0525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting node embeddings"
      ],
      "metadata": {
        "id": "RQHbFsalWPdA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7IP1ORnt26j"
      },
      "source": [
        "x_emb_in, x_emb_out = gcn_model.in_out_tensors()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2p2EA_bt_lS"
      },
      "source": [
        "x_out = tf.squeeze(x_emb_out, axis=0)\n",
        "emb_model = Model(inputs=x_emb_in, outputs=x_out)\n",
        "all_embeddings = emb_model.predict(fullbatch_generator.flow(G.nodes()))\n",
        "all_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}